{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 2\n",
    "Assigment is in the intersection of Named Entity Recognition and Dependency Parsing.\n",
    "\n",
    "0. Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "    - report token-level performance (per class and total)\n",
    "        - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy) \n",
    "    - report CoNLL chunk-level performance (per class and total);\n",
    "        - precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total  \n",
    "\n",
    "1. Grouping of Entities.\n",
    "Write a function to group recognized named entities using `noun_chunks` method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks). Analyze the groups in terms of most frequent combinations (i.e. NER types that go together). \n",
    "\n",
    "2. One of the possible post-processing steps is to fix segmentation errors.\n",
    "Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Start\n",
    "\n",
    "- First we need to import everything we will ned to use. \n",
    "\n",
    "- I imported every function from the \"conll.py\" file, because I decided to use the \"read_corpus_conll\" function to read the conll2003 file. \n",
    "\n",
    "- I find alse the Class WhitespaceTokenizer in the SpaCy ducumentation to have a more effective tokenization from string \n",
    "\n",
    "- I will use classification_report of sklearn.metrics to evaluete the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from conll import *\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# taken from https://spacy.io/usage/linguistic-features#tokenization\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        return Doc(self.vocab, words=words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('SOCCER NN B-NP O',), ('- : O O',), ('JAPAN NNP B-NP B-LOC',), ('GET VB B-VP O',), ('LUCKY NNP B-NP O',), ('WIN NNP I-NP O',), (', , O O',), ('CHINA NNP B-NP B-PER',), ('IN IN B-PP O',), ('SURPRISE DT B-NP O',), ('DEFEAT NN I-NP O',), ('. . O O',)], [('Nadim NNP B-NP B-PER',), ('Ladki NNP I-NP I-PER',)]]\n"
     ]
    }
   ],
   "source": [
    "def get_sents(path):\n",
    "    \n",
    "    sents=read_corpus_conll(path)\n",
    "    \n",
    "    sents.remove(sents[0]) #removing '-DOCSTART- -X- -X- O' line\n",
    "    #print(sents)\n",
    "    return sents\n",
    "    \n",
    "path1='../src/conll2003/test.txt'\n",
    "path2='../src/conll2003/train.txt'\n",
    "\n",
    "sents=get_sents(path1)\n",
    "print(sents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT\n",
      "[['SOCCER', 'NN', 'B-NP', 'O'], ['-', ':', 'O', 'O'], ['JAPAN', 'NNP', 'B-NP', 'B-LOC'], ['GET', 'VB', 'B-VP', 'O'], ['LUCKY', 'NNP', 'B-NP', 'O'], ['WIN', 'NNP', 'I-NP', 'O'], [',', ',', 'O', 'O'], ['CHINA', 'NNP', 'B-NP', 'B-PER'], ['IN', 'IN', 'B-PP', 'O'], ['SURPRISE', 'DT', 'B-NP', 'O'], ['DEFEAT', 'NN', 'I-NP', 'O']]\n"
     ]
    }
   ],
   "source": [
    "def get_sent_string(corpus):\n",
    "    temp=[]\n",
    "    temp2=[]\n",
    "    t_sent=[] #['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT']..\n",
    "    f_sent=[] #'SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT'\n",
    "    s_iobsent=[] #['SOCCER', 'NN', 'B-NP', 'O'], ['-', ':', 'O', 'O'] ...\n",
    "    for s in sents: \n",
    "        for w in s:\n",
    "            if (w[0].split()[0]!='.'):\n",
    "                temp.append(w[0].split()[0])\n",
    "                temp2.append(w[0].split())\n",
    "\n",
    "            else:\n",
    "                t_sent.append(temp)\n",
    "                s_iobsent.append(temp2)\n",
    "                temp=[]\n",
    "                temp2=[]\n",
    "\n",
    "    for s in t_sent:\n",
    "        f_sent.append(' '. join(s))\n",
    "\n",
    "    return f_sent,s_iobsent\n",
    "\n",
    "s, iob= get_sent_string(sents)\n",
    "\n",
    "print (s[0])\n",
    "print (iob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create doc for all the sentencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "\n",
    "for i in s: \n",
    "    doc=nlp(i)\n",
    "    sentences.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From SpaCy format to ConLL \n",
    "\n",
    "1. There are four types of phrases in CoNLL format: person names (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC)\n",
    "\n",
    "\n",
    "2. Spacy (Named Entity Recognition, OntoNotes 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf)\n",
    "    \n",
    "    - \"PERSON\": \"People, including fictional\",\n",
    "    - \"NORP\": \"Nationalities or religious or political groups\",\n",
    "    - \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n",
    "    - \"FAC\": \"Buildings, airports, highways, bridges, etc.\",\n",
    "    - \"ORG\": \"Companies, agencies, institutions, etc.\",\n",
    "    - \"GPE\": \"Countries, cities, states\",\n",
    "    - \"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",\n",
    "    - \"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",\n",
    "    - \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n",
    "    - \"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n",
    "    - \"LAW\": \"Named documents made into laws.\",\n",
    "    - \"LANGUAGE\": \"Any named language\",\n",
    "    - \"DATE\": \"Absolute or relative dates or periods\",\n",
    "    - \"TIME\": \"Times smaller than a day\",\n",
    "    - \"PERCENT\": 'Percentage, including \"%\"',\n",
    "    - \"MONEY\": \"Monetary values, including unit\",\n",
    "    - \"QUANTITY\": \"Measurements, as of weight or distance\",\n",
    "    - \"ORDINAL\": '\"first\", \"second\", etc.',\n",
    "    - \"CARDINAL\": \"Numerals that do not fall under another type\",\n",
    "\n",
    "\n",
    "\n",
    "3. I defined the function creating tha hypotesis that get in input a list of strings anche return a list of tokenised and elaborate tuples \n",
    "    - input: ['SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT' , '...']\n",
    "    - output: [[(SOCCER, 'O'), (-, 'O'), (JAPAN, 'O'), (GET, 'O'), (LUCKY, 'O'), (WIN, 'O'), (,, 'O'), (CHINA, 'B-LOC'), (IN, 'O'), (SURPRISE, 'O'), (DEFEAT, 'O')], [...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(SOCCER, 'O'), (-, 'O'), (JAPAN, 'O'), (GET, 'O'), (LUCKY, 'O'), (WIN, 'O'), (,, 'O'), (CHINA, 'B-LOC'), (IN, 'O'), (SURPRISE, 'O'), (DEFEAT, 'O')], [(Nadim, 'B-ORG'), (Ladki, 'I-ORG'), (AL-AIN, 'O'), (,, 'O'), (United, 'B-LOC'), (Arab, 'I-LOC'), (Emirates, 'I-LOC'), (1996-12-06, 'O'), (Japan, 'B-LOC'), (began, 'O'), (the, 'O'), (defence, 'O'), (of, 'O'), (their, 'B-MISC'), (Asian, 'I-MISC'), (Cup, 'I-MISC'), (title, 'O'), (with, 'O'), (a, 'O'), (lucky, 'O'), (2-1, 'B-MISC'), (win, 'O'), (against, 'O'), (Syria, 'B-LOC'), (in, 'O'), (a, 'O'), (Group, 'B-ORG'), (C, 'I-ORG'), (championship, 'O'), (match, 'O'), (on, 'O'), (Friday, 'B-MISC')]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = {\n",
    "    'PERSON': 'PER',\n",
    "    'NORP': 'MISC',\n",
    "    'FAC': 'ORG',\n",
    "    'ORG': 'ORG',\n",
    "    'GPE': 'LOC',\n",
    "    'LOC': 'LOC',\n",
    "    'PRODUCT': 'MISC',\n",
    "    'EVENT': 'MISC',\n",
    "    'WORK_OF_ART': 'PER' ,\n",
    "    'LAW': 'MISC',\n",
    "    'LANGUAGE': 'MISC',\n",
    "    'DATE': 'MISC',\n",
    "    'TIME': 'MISC',\n",
    "    'PERCENT': 'MISC',\n",
    "    'MONEY': 'MISC',\n",
    "    'QUANTITY': 'MISC',\n",
    "    'ORDINAL': 'MISC',\n",
    "    'CARDINAL': 'MISC',\n",
    "}\n",
    "\n",
    "\n",
    "def get_hyp(sents):\n",
    "\n",
    "    \n",
    "    hip=[]\n",
    "    temp=[]\n",
    "\n",
    "    for s in sents:\n",
    "        for t in s:\n",
    "            if t.ent_type_ in dictionary.keys():\n",
    "                x=t.ent_iob_+ '-'+dictionary[t.ent_type_]\n",
    "                temp.append((t, x))\n",
    "            else: \n",
    "                temp.append((t, t.ent_iob_))\n",
    "        hip.append(temp)\n",
    "        temp=[]\n",
    "\n",
    "    return hip\n",
    "\n",
    "hyp=get_hyp(sentences)\n",
    "print (hyp[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting 'refs'\n",
    "\n",
    "from the function 'get_sent_string(corpus)' I returned the concatenized string but alza the plit string with all the other parameters, so is I take that variable (iob) and remove the part from each list of each sentences that I don't need for the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SOCCER', 'NN', 'B-NP', 'O'], ['-', ':', 'O', 'O'], ['JAPAN', 'NNP', 'B-NP', 'B-LOC'], ['GET', 'VB', 'B-VP', 'O'], ['LUCKY', 'NNP', 'B-NP', 'O'], ['WIN', 'NNP', 'I-NP', 'O'], [',', ',', 'O', 'O'], ['CHINA', 'NNP', 'B-NP', 'B-PER'], ['IN', 'IN', 'B-PP', 'O'], ['SURPRISE', 'DT', 'B-NP', 'O'], ['DEFEAT', 'NN', 'I-NP', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print (iob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SOCCER', 'O'], ['-', 'O'], ['JAPAN', 'B-LOC'], ['GET', 'O'], ['LUCKY', 'O'], ['WIN', 'O'], [',', 'O'], ['CHINA', 'B-PER'], ['IN', 'O'], ['SURPRISE', 'O'], ['DEFEAT', 'O']]\n"
     ]
    }
   ],
   "source": [
    "def get_refs(corpus):\n",
    "    sent, iob= get_sent_string(corpus)\n",
    "    for s in iob: \n",
    "        for t in s: \n",
    "            t.remove(t[1])\n",
    "            t.remove(t[1])\n",
    "\n",
    "    return iob\n",
    "\n",
    "refs=get_refs(sents)\n",
    "print (refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1) Token-Level Accuracy\n",
    "\n",
    "- I take the iob information in the hyp and refs, save it in two list of \"output\" and \"predicted\", I then use classification_ report to confront them \n",
    "\n",
    "(As for piazza question of \"Steve Azzolin\" I used for example B-ORG as a different class wrt I-ORG and I solso keeped the tagged O)\n",
    "\n",
    "- From sklearn: \n",
    "    - The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "    - The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "    - The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "        - F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score       support\n",
      "B-LOC          0.791753  0.690647  0.737752   1668.000000\n",
      "B-MISC         0.106371  0.547009  0.178108    702.000000\n",
      "B-ORG          0.502083  0.290187  0.367799   1661.000000\n",
      "B-PER          0.773344  0.599258  0.675261   1617.000000\n",
      "I-LOC          0.593750  0.591440  0.592593    257.000000\n",
      "I-MISC         0.051990  0.375000  0.091319    216.000000\n",
      "I-ORG          0.428858  0.512575  0.466994    835.000000\n",
      "I-PER          0.802446  0.737889  0.768815   1156.000000\n",
      "O              0.938765  0.861456  0.898451  36927.000000\n",
      "accuracy       0.806235  0.806235  0.806235      0.806235\n",
      "macro avg      0.554373  0.578385  0.530788  45039.000000\n",
      "weighted avg   0.879129  0.806235  0.836746  45039.000000\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(hyp, refs):\n",
    "    pred = []\n",
    "    output=[]\n",
    "    \n",
    "    for s in refs:\n",
    "        for t in s:\n",
    "              output.append(t[1])\n",
    "    \n",
    "    for s in hyp:\n",
    "        for t in s:\n",
    "              pred.append(t[1])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return output,pred\n",
    "\n",
    "out, pred= get_accuracy (hyp, refs)\n",
    "report = classification_report(out, pred, output_dict=True)\n",
    "pd_tbl = pd.DataFrame(report).transpose()\n",
    "#pd_tbl.round(decimals=3)\n",
    "print (pd_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2) Chunk level performance\n",
    "Using the evaluate function in conll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.441</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.741</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.727</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.104</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.174</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.393</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.443</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "ORG    0.441  0.255  0.323  1661\n",
       "PER    0.741  0.574  0.647  1617\n",
       "LOC    0.780  0.680  0.727  1668\n",
       "MISC   0.104  0.536  0.174   702\n",
       "total  0.393  0.507  0.443  5648"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = evaluate (refs, hyp)\n",
    "pd_tbl = pd.DataFrame().from_dict(report, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Grouping of Entities\n",
    "\n",
    "- I create three function (I need both because as demostrated below the numeber of entities with doc.ents and noun_chuck are differente, I use noun_chuck and discard the one that are found not found in the doc.ents and then store in one arrey ) (Also for the comment in Piazza were professor Stepanov wrote \"pay attention... some entities may be not within noun_chunk spans\"): \n",
    "    - the first to retive the entities with the function noun_chucks, and I return them \n",
    "    - the second to extract the entities from a doc with 'doc.ents' and confront with the previius list, and then I return them\n",
    "      \n",
    "- the peculiar thing is thaht noun_chuck return a Span object: \n",
    "    - Token vs Span: From spaCy's documentation, a Token represents a single word, punctuation symbol, whitespace, etc. from a document, while a Span is a slice from the document. In other words, a Span is an ordered sequence of Tokens. \n",
    "    - So u can acces the entity type with \"span.root.ent_type\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent_doc(sents):\n",
    "    ent=tuple()\n",
    "    for d in sents: \n",
    "        for e in d.ents: \n",
    "            ent.append(e)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent_noun(sents):\n",
    "    ent=[]\n",
    "    for d in sents: \n",
    "        for e in d.noun_chunks: \n",
    "            for s in e.ents: \n",
    "                ent.append(s)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a=get_ent_doc(sentences)\n",
    "b=get_ent_noun(sentences)\n",
    "\n",
    "print (len(a)==len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the implementation of \"get_ent(sents)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent(sents):\n",
    "    ent=[]\n",
    "    temp = []\n",
    "    check=get_ent_noun(sents)\n",
    "    for d in sents:\n",
    "        for e in d.noun_chunks:\n",
    "            for s in e.ents:\n",
    "                if s in check:\n",
    "                    temp.append(s.root.ent_type_)\n",
    "                    \n",
    "                    check.remove(s)\n",
    "            ent.append(temp)\n",
    "            temp=[]\n",
    "    \n",
    "    \n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(ent):\n",
    "    freq={}\n",
    "    for x in ent:\n",
    "        if tuple(x)!=():\n",
    "            if(tuple(x) not in freq):\n",
    "                freq[tuple(x)] = 1\n",
    "            else:\n",
    "                freq[tuple(x)] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GPE',)  :  1121\n",
      "('PERSON',)  :  894\n",
      "('ORG',)  :  651\n",
      "('CARDINAL',)  :  510\n",
      "('DATE',)  :  472\n",
      "('NORP',)  :  273\n",
      "('CARDINAL', 'PERSON')  :  102\n",
      "('ORDINAL',)  :  87\n",
      "('PERCENT',)  :  50\n",
      "('EVENT',)  :  49\n",
      "('MONEY',)  :  48\n",
      "('TIME',)  :  42\n",
      "('LOC',)  :  40\n",
      "('NORP', 'PERSON')  :  38\n",
      "('QUANTITY',)  :  36\n",
      "('GPE', 'PERSON')  :  35\n",
      "('CARDINAL', 'ORG')  :  35\n",
      "('CARDINAL', 'GPE')  :  26\n",
      "('CARDINAL', 'NORP')  :  22\n",
      "('CARDINAL', 'CARDINAL')  :  20\n",
      "('ORG', 'PERSON')  :  20\n",
      "('GPE', 'GPE')  :  18\n",
      "('FAC',)  :  18\n",
      "('PRODUCT',)  :  17\n",
      "('GPE', 'ORG')  :  16\n",
      "('GPE', 'CARDINAL')  :  12\n",
      "('WORK_OF_ART',)  :  10\n",
      "('DATE', 'EVENT')  :  9\n",
      "('PERSON', 'PERSON')  :  9\n",
      "('ORG', 'ORG')  :  9\n",
      "('DATE', 'TIME')  :  7\n",
      "('PERSON', 'GPE')  :  7\n",
      "('GPE', 'PRODUCT')  :  7\n",
      "('GPE', 'DATE')  :  6\n",
      "('NORP', 'ORDINAL')  :  6\n",
      "('GPE', 'ORDINAL')  :  5\n",
      "('CARDINAL', 'GPE', 'GPE')  :  5\n",
      "('ORG', 'GPE')  :  5\n",
      "('ORG', 'DATE')  :  5\n",
      "('CARDINAL', 'CARDINAL', 'CARDINAL')  :  5\n",
      "('NORP', 'ORG')  :  5\n",
      "('DATE', 'ORG')  :  4\n",
      "('DATE', 'PERSON')  :  4\n",
      "('ORG', 'NORP')  :  4\n",
      "('LANGUAGE',)  :  4\n",
      "('ORDINAL', 'TIME')  :  3\n",
      "('CARDINAL', 'CARDINAL', 'PERSON')  :  3\n",
      "('LANGUAGE', 'ORDINAL')  :  3\n",
      "('DATE', 'LANGUAGE', 'ORDINAL')  :  3\n",
      "('GPE', 'LOC')  :  3\n",
      "('DATE', 'NORP')  :  3\n",
      "('CARDINAL', 'CARDINAL', 'ORG')  :  3\n",
      "('GPE', 'CARDINAL', 'NORP', 'PERSON')  :  2\n",
      "('CARDINAL', 'CARDINAL', 'GPE')  :  2\n",
      "('PERSON', 'ORG')  :  2\n",
      "('DATE', 'CARDINAL')  :  2\n",
      "('CARDINAL', 'ORG', 'GPE')  :  2\n",
      "('GPE', 'CARDINAL', 'ORG')  :  2\n",
      "('GPE', 'GPE', 'GPE')  :  2\n",
      "('ORG', 'CARDINAL', 'ORG')  :  2\n",
      "('CARDINAL', 'CARDINAL', 'NORP')  :  2\n",
      "('ORDINAL', 'EVENT')  :  2\n",
      "('ORDINAL', 'PERSON')  :  2\n",
      "('QUANTITY', 'QUANTITY')  :  2\n",
      "('GPE', 'CARDINAL', 'NORP')  :  2\n",
      "('GPE', 'CARDINAL', 'GPE')  :  2\n",
      "('DATE', 'NORP', 'PERSON')  :  2\n",
      "('NORP', 'NORP')  :  2\n",
      "('GPE', 'EVENT')  :  2\n",
      "('CARDINAL', 'EVENT')  :  2\n",
      "('GPE', 'FAC')  :  2\n",
      "('QUANTITY', 'ORDINAL')  :  2\n",
      "('PERSON', 'CARDINAL')  :  2\n",
      "('ORG', 'CARDINAL')  :  2\n",
      "('ORDINAL', 'NORP')  :  1\n",
      "('PERSON', 'PERSON', 'PERSON')  :  1\n",
      "('ORG', 'QUANTITY')  :  1\n",
      "('PERSON', 'GPE', 'ORG')  :  1\n",
      "('PERSON', 'CARDINAL', 'PERSON')  :  1\n",
      "('ORG', 'GPE', 'PERSON')  :  1\n",
      "('ORG', 'ORG', 'PERSON')  :  1\n",
      "('CARDINAL', 'CARDINAL', 'CARDINAL', 'GPE')  :  1\n",
      "('CARDINAL', 'ORG', 'ORG')  :  1\n",
      "('CARDINAL', 'ORG', 'CARDINAL')  :  1\n",
      "('PERSON', 'PERSON', 'CARDINAL', 'PERSON')  :  1\n",
      "('DATE', 'GPE')  :  1\n",
      "('GPE', 'DATE', 'ORG')  :  1\n",
      "('CARDINAL', 'ORG', 'PERSON')  :  1\n",
      "('NORP', 'GPE')  :  1\n",
      "('PERSON', 'NORP')  :  1\n",
      "('LOC', 'DATE')  :  1\n",
      "('DATE', 'FAC')  :  1\n",
      "('GPE', 'PERSON', 'CARDINAL', 'GPE')  :  1\n",
      "('LOC', 'ORDINAL')  :  1\n",
      "('GPE', 'NORP', 'PERSON')  :  1\n",
      "('CARDINAL', 'GPE', 'PERSON')  :  1\n",
      "('NORP', 'LOC')  :  1\n",
      "('LOC', 'ORG')  :  1\n",
      "('ORDINAL', 'GPE')  :  1\n",
      "('NORP', 'CARDINAL', 'CARDINAL')  :  1\n",
      "('CARDINAL', 'PERCENT')  :  1\n",
      "('DATE', 'WORK_OF_ART')  :  1\n",
      "('TIME', 'TIME')  :  1\n",
      "('CARDINAL', 'DATE')  :  1\n",
      "('CARDINAL', 'CARDINAL', 'DATE')  :  1\n",
      "('MONEY', 'CARDINAL', 'CARDINAL', 'ORG')  :  1\n",
      "('CARDINAL', 'CARDINAL', 'PRODUCT')  :  1\n",
      "('CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL')  :  1\n",
      "('GPE', 'ORG', 'FAC')  :  1\n",
      "('GPE', 'GPE', 'PERSON')  :  1\n",
      "('CARDINAL', 'GPE', 'TIME')  :  1\n",
      "('CARDINAL', 'LOC')  :  1\n",
      "('ORG', 'CARDINAL', 'NORP')  :  1\n",
      "('GPE', 'NORP')  :  1\n",
      "('ORG', 'WORK_OF_ART')  :  1\n",
      "('ORG', 'NORP', 'PERSON')  :  1\n",
      "('GPE', 'ORDINAL', 'PERSON')  :  1\n",
      "('ORG', 'MONEY')  :  1\n",
      "('PERSON', 'MONEY')  :  1\n",
      "('MONEY', 'PRODUCT')  :  1\n",
      "('CARDINAL', 'DATE', 'PERSON')  :  1\n",
      "('PERSON', 'GPE', 'CARDINAL', 'PERSON')  :  1\n",
      "('ORDINAL', 'ORG')  :  1\n",
      "('EVENT', 'CARDINAL')  :  1\n",
      "('CARDINAL', 'QUANTITY')  :  1\n",
      "('GPE', 'PERSON', 'ORG')  :  1\n",
      "('CARDINAL', 'NORP', 'ORG')  :  1\n",
      "('DATE', 'CARDINAL', 'ORG')  :  1\n",
      "('GPE', 'ORG', 'DATE')  :  1\n"
     ]
    }
   ],
   "source": [
    "ent=get_ent(sentences)\n",
    "f= get_freq(ent)\n",
    "sorted_dict = dict( sorted(f.items(),\n",
    "                           key=lambda item: item[1],\n",
    "                           reverse=True))\n",
    "\n",
    "for key, value in sorted_dict.items():\n",
    "    print(key, ' : ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Post-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
